include "base_train_config.conf"

ru_instruct_gpt4_ddp = ${base_train} {
  "max_tokens": 1024,

  "train_loader": {
    "batch_size": 1,
    "num_workers": 8,
  },

  "valid_loader": {
    "batch_size": 1,
    "num_workers": 8,
  },

  "optimizer": {
    "lr": 1e-4
  },

  "model_type": "bfloat16",

  "lora": true,
  "lora_config": {
    "r": 16,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj"]
  },

  "scheduler": "CyclicLR",
  "CyclicLR": {
    "base_lr": 2e-6,
    "max_lr": 2e-4,
    "step_size_up": 100,
    "mode": "exp_range",
    "gamma": 0.999,
    "cycle_momentum": false
  },

  "pretrained_path": "/s/ls4/groups/g0126/transformers_models/mistralai/Mistral-7B-Instruct-v0.2",
  "save_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/models/ruMistral-7B-ddp",
  "data_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/data/prep/ru_instruct_gpt4.csv",
  "log_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/log/mistral-7B-lora",
  "experiment_name": "lora-16",
  "valid_path": null,
  "valid_size": 0.05,

  "train_epochs": 5
}