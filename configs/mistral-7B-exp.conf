include "base_train_config.conf"

ru_instruct_gpt4_ddp = ${base_train} {
  # Длина (в токенах) входного текста
  "max_tokens": 1024,

  # Настройки обучающей выборки
  "train_epochs": 3,
  "train_loader": {
    "batch_size": 1,
    "num_workers": 12,
    "pin_memory": true
  },

  # Настройки тестовой выборки, если нет, то данные на валид берутся из трейна см. ddp_train_script.py
  "valid_path": null,
  "valid_size": 0.05,
  "valid_loader": {
    "batch_size": 1,
    "num_workers": 12,
    "pin_memory": true
  },

  # Настройки оптимизатора
  "optimizer": {
    "lr": 1e-3
  },

  # Точность весов
  "model_type": "bfloat16",

  # Сохраняет градиенты в жесткий диск, если True можно брать больший batch_size, но будет медленне учиться
  "gradient_checkpointing": false,

  # Настройки адаптера LoRA
  "lora": true,
  "lora_config": {
    "r": 16,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "lm_head"]
  },

  # Выбор планировщика скорости обучения
  "scheduler": "CyclicLR",

  "CyclicLR": {
    "base_lr": 2e-5,
    "max_lr": 1e-3,
    "step_size_up": 50,
    "mode": "exp_range",
    "gamma": 0.999,
    "cycle_momentum": false
  },
  "CosineAnnealingLR": {
    "T_max": 500,
    "eta_min": 1e-6
  },
  "LambdaLR": 0.999,

  # Настройка путей
  "pretrained_path": "/s/ls4/groups/g0126/transformers_models/mistralai/Mistral-7B-Instruct-v0.2",
  "save_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/models/Mistral-7B-lora-lm_head",
  "data_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/data/prep/ru_instruct_gpt4.csv",

  # Логирование mlflow
  "log_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/log/mistral-7B-lora",
  "experiment_name": "lora-16"
}