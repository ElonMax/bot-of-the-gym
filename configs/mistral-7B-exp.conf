include "base_train_config.conf"

ru_instruct_gpt4_ddp = ${base_train} {
  "max_tokens": 1024,

  "train_loader": {
    "batch_size": 1,
    "num_workers": 12,
    "pin_memory": true
  },

  "valid_loader": {
    "batch_size": 1,
    "num_workers": 12,
    "pin_memory": true
  },

  "optimizer": {
    "lr": 1e-3
  },

  "model_type": "bfloat16",
  "gradient_checkpointing": false,

  "lora": true,
  "lora_config": {
    "r": 16,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "lm_head"]
  },

  "scheduler": "CyclicLR",
  "CyclicLR": {
    "base_lr": 2e-5,
    "max_lr": 1e-3,
    "step_size_up": 50,
    "mode": "exp_range",
    "gamma": 0.999,
    "cycle_momentum": false
  },
  "CosineAnnealingLR": {
    "T_max": 500,
    "eta_min": 1e-6
  },
  "LambdaLR": 0.999,

  "pretrained_path": "/s/ls4/groups/g0126/transformers_models/mistralai/Mistral-7B-Instruct-v0.2",
  "save_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/models/Mistral-7B-lora-lm_head",
  "data_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/data/prep/ru_instruct_gpt4.csv",
  "log_path": "/s/ls4/users/cappukan/projects/bot-of-the-gym/log/mistral-7B-lora",
  "experiment_name": "lora-16",
  "valid_path": null,
  "valid_size": 0.05,

  "train_epochs": 3
}